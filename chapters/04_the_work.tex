\fancychapter{Clustering Tweets with Self-Organizing Maps}

Tweets cathegorization with \ac{SOMs} requires a dataset to work on. Due to sistematic API restrictions, which where implemented in order to protect Twitter business model, gathering a dataset that directly maps to the social network is a great chalenge by itself. 
In order to cathegorize tweets with \ac{SOMs} first we used a dataset provided by INESC-ID. The dataset had almost 1TB of data in \ac{JSON} format.

Each tweet is comprised of multiple paramters. Figure~\ref{fig:json_tweet} shows how a tweet is represented in \ac{JSON} format. Inside the tweet there is also information about the user whom created the tweet, shown in Figure~\ref{fig:json_tweet_user} and enteties shown in Figure~\ref{fig:tweet_enteties}.

As can be seen in Figure~\ref{fig:json_tweet_user} no information about the social relations of the user which emited the tweet are present. Therefor in order to retrieve the social network in which a user is contained, it will be necessary to connect to the Twitter API. Crawling twitter is discussed in further depth in Chapter~\ref{chap:crawling_twitter}.  

\input{./images/json.tex}
\input{./images/tweet_json_enteties.tex}
\input{./images/user_info.tex}

In order to better understand the dataset at hand, all the \ac{JSON} files where converted into \ac{CSV}in a way to reduce the size of the dataset. While tweets where being converted, \ac{URL} where removed due to the fact that most of them where minified in order to fit in less that 140 characters. Also, all tweets that where not identified as being in english where also removed. The tweet shown in \ac{JSON} format in Figure~\ref{fig:json_tweet} is converted to \ac{CSV} in Figure~\ref{fig:csv_tweet}.
%TODO find how many tweets have en user, and are not in english
%     count amount of different words with\without en filter and bloom filters
%     test whatlanguage to see if it is good to ientify language on tweets
%     size reduction
\input{./images/tweet_csv.tex}
Identifying tweets that where not in the english language was done through the usage of Ruby library called whatlanguage~\footnote{https://github.com/peterc/whatlanguage}, which tries to identify one language through Bloom Filters. Inside the tweet there is a field which identifies the user language, we found that x is not acurate. Removing tweets that weren't in the english language reduced the amount of different words in x and therefor will reduce the dimensional size of the \ac{SOM}.

The initial dataset features is described table x
 .
\input{./tables/inesc_dataset.tex}

Work done on the INESC twitter dataset with SOMs.
SOM implementations used, what where their strong points and weaknesses
SVM Dimension reduction and text treatment: compare multiple aproaches to reduce the svm size of tweets without losing relevant information

\fancychapter{Crawling Twitter for Social Relations}
\label{chap:crawling_twitter}
Explain the limitations of the INESC tweet dataset: crawlled by hashtag, social connections can only be obtained through connections to the twitter API, a lot of the tweets had no active users etc. 

\fancychapter{SOM Framework}
Explain what got me to create my own ruby library: everybody is making their own SOM algorithms ( ex.: websom, hsom etc ). Most implementations want to get as close to the metal as possible in order to deliver faster trainings, which makes the lybraries hard to modify. SOM framework is an modular implementation of the SOM algorithm in an higher level programing language which makes it easier to construct and test new SOM algorithms.

\fancychapter{Homophilic SOM}
The default \ac{SOM} algorithm has no idea whatsoever of the social connections between the tweets, it simply looks at the binary vectors that represent sentences and assigns it to the most similar neuron.

In order to better categorize socially connected data, we propose some alterations to the \ac{SOM} algorithm in order to make it aware of the social connections between the tweets, and therefor better represent the homophilic behavior present on social networks.

{\color{red} insert homophilic som algorith here}

\chapter{Algorithm changes}
\label{cha:algorithm_changes}

\subsection{Output Space}
\label{sub:output_space}
The outputs space is the zone on the \ac{SOM} algorithm where the neurons reside. It works like a cortex where neurons are scattered in a geometric fashion, generally a square. The output space is generally initialized with random values, with a relatively high learning rate, and also a relatively high number of epochs. The algorithm is made this way in order to be able to identify any type of data that can be represented as vectors.

First we will try to change the output space to better resemblance the social network. In order to do this, the squared grid that defines the output space was changed by the social network connections, and the neurons, are represented by a social network user. This changes are applied in the following way:
\begin{itemize}
  \item Each neuron is comprised of the text from all the tweets that he authored.
  \item Each neuron has a unique id, and stores the ids of his followers and followees that are present in the output space.
  \item During the learning phase, the radius will be defined as the maximum number of hops separating the winning neuron and followers/followees of followers/followees. 
  \item Each neuron will cache followers/followees of a follower/followee to a specified depth level, for performance purposes. 
\end{itemize}

{\color{red} insert image of the output space with social features vrs tipical output space}

\subsection{Learning Phase}
\label{sub:learning_phase}
Like in the default \ac{SOM} the learning phase is where the output space is trainned in order to organize the input data into clusters. Since this algorithm is specific to cathegorize tweets using social network features, the learning rate, radius and number of epochs used can be greatly reduced in order for the algorithm to converge. The learning phase operates in the following way:

\begin{itemize}
  \item The distance between the input pattern and all the neurons is calculated. The neuron closest to the input pattern is considered the winning neuron.
  \item When the winning neuron is selected, he and his social neighbors within k hops, update their representations in the input space and move closer to the input patter. The Gaussian function (Func~\ref{eq:gaussian}) is also used in here in order for the neighbors that are closer to the input pattern be significantly more influenced by the input pattern, while the neurons further away are less influenced. 
    \item 
    
  
\end{itemize}

{\color{red} Link to the learning phase in the algorithm on the main chapter, add images of the trainning model }
