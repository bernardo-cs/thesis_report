\fancychapter{Clustering Tweets with Self Organizing Maps}
\label{ch:clustering_tweets}

\section{Introduction}
\label{sec:adapting_the_som_to_the_social_web}

\section{Twitter Data}
\label{sec:crawling_twitter}
Twitter is a social network website and mobile app, where users are able to share whats on their mind with less than 140 characters. Due to its limitations, twitter users started to adopt their own kind of language on the social network, sharing shortened \ac{URL}, and tagging topics with a word preceded with an hashtag -- \# -- became so popular, that it was eventually implemented into twitter itself. Nowadays it is possible to monitor events through hashtags selections and links shared on the mobile app are automatically shortened.   

With the rise of smartphones and decent prices for mobile Internet access, and people becoming always online, GPS coordinates where also added to tweets. In fact a tweet nowadays has a massive amount of information, as can be seen in Figure~\ref{fig:json_tweet}.

\input{./images/json.tex}

There are two main ways to gather data from twitter: Crawl twitter HTML pages and scrap the intended information, access through the twitter API.

Crawling web pages is done through the analysis of HTML documents generated by twitter servers. Due to specific semantic rules, it is possible to gather almost all information that is possible to have access through the twitter API. Even though writing an HTML crawler is not particularly complex, specially through the usage of open source tools like nokogiri \footnote{http://www.nokogiri.org/} or beautiful soup \footnote{http://www.crummy.com/software/BeautifulSoup/}, Twitter specifically asks to not be crawled in some parts of their \ac{URL}, as can be seen in Figure~\ref{fig:twitterrobots}. 

Basically the restrictions that twitter asks on his robots.txt, only allows for search results, and hashtag searches to be monitored, which is pretty limiting.

\begin{figure}[htpb]
  \centering
  \begin{boxedverbatim}
  # Every bot that might possibly read and respect this file.
  User-agent: *
  Allow: /?lang=
  Allow: /hashtag/*?src=
  Allow: /search?q=%23
  Disallow: /search/realtime
  Disallow: /search/users
  Disallow: /search/*/grid

  Disallow: /*?
  Disallow: /*/followers
  Disallow: /*/following

  Disallow: /account/not_my_account

  Disallow: /oauth
  Disallow: /1/oauth
  \end{boxedverbatim}
  \caption{Twitter robots.txt piece where it is possible to see what should and shouldn't be crawled}
  \label{fig:twitterrobots}
\end{figure}

When accessing twitter through their API, authentication is required as since version 1.1. The authentication mechanism at hand, is used to limit the amount of information users can gather from twitter. 
The API itself is divided in the streaming -- used for subscribing directly to twitter public, user or site streams --  and REST -- used for programmatic access to read and write to the twitter API.  

The streaming API is extremely useful for building dataset based on keywords ,searches and entities. Since their API limit is based on levels, and the default level lets an endpoint track up to 400 words, and 5000 user ids as long as the amount of tweets streamed to the endpoint doesn't  surpass the 1\% of the total amount of tweets Twitter is currently streaming. Although, if some wrong terms are monitored, the amount of spam crawled can be huge, specially when monitoring public entities or trending hashtags. 

The REST API can be used to get all information that is available on twitter by the time the request is sent. REST API limits are much greater than the ones applied at the streaming API. The basic rules are 15 minutes windows per endpoint where 15 requests can be made. There are some exceptions to these rules and those can be found on twitter documentation \footnote{https://dev.twitter.com/rest/public}

Due to the amount of specificity allowed by the REST API, it is better suited to create datasets that mimic the way twitter data is interconnected, like getting users and their followers, as well as their tweets.

\subsection{Crawling Twitter for Social Relations}
\label{sub:crawling_twitter_for_social_relations}
When trying to find clusters of topics with \ac{SOM}, integrating the social network in the output space the need for a dataset which had the social connections between the authors of the tweets arose.

Given the fact that the social relations where required, we opted for making a crawler based on the REST API. Due to the fact that twitter API rate limits would be achieved with some ease, the crawler should be prepared to achieve this maximum amount of requests per 15 minute window and wait for fifteen minutes. 

Also, at a given time, the crawler should be able to serialize its state in order to able to resume crawling in case its has to stop at any given worldly circumstances. 



\section{SOM}

\subsection{Clustering Tweets}
\label{sub:clustering_tweets}

In order to use \ac{SOM} to cluster tweets, first the tweets need to be converted into \ac{VSM}. Given the fact the tweets are often misspelled 

\subsection{Extensible SOM Library}
\label{sub:extensible_som_library}

When researching ways to extend the \ac{SOM} algorithm, in order to add social features to the learning process. I found that the number of \ac{SOM} libraries was not very extense. Even though, programing languages often used in \ac{ML} and Data Mining, such as Python or C++, have their how implementation of the \ac{SOM} algorithm. I've found that most of these libraries are made in such a way to be extremely fast, in order to take as much advantage from the hardware they are running on as possible. They often lack the modularity needed to adapt the \ac{SOM} algorithm to specific problems.

The \ac{SOM} algorithm has been changed many times in order to better categorize data with specific features, for example Geo-SOM was described in Subsection~\ref{sub:types_of_soms}, the Growing Hierarchical SOM~\cite[]{1058070}, the time adaptive SOM~\cite[]{1187438}, the Ontological SOM~\cite[]{5446427}, and the list goes on\dots  

In order to create the homophilic SOM, described in Section~\ref{sec:algorithm_changes} we first created a SOM framework that is easy to extend due to be fully object oriented, scripted --- even though it can be compiled to run on the JVM --- and without C extensions.


\subsection{Clustering Socially Connected Data}
The default \ac{SOM} algorithm has no idea whatsoever of the social connections between the tweets, it simply looks at the binary vectors that represent sentences and assigns it to the most similar neuron.

In order to better categorize socially connected data, we propose some alterations to the \ac{SOM} algorithm in order to make it aware of the social connections between the tweets, and therefor better represent the homophilic behavior present on social networks.

{\color{red} insert homophilic som algorith here}

\section{Homophilic SOM Definition}
\label{sec:algorithm_changes}

\subsection{Output Space}
\label{sub:output_space}
The outputs space is the zone on the \ac{SOM} algorithm where the neurons reside. It works like a cortex where neurons are scattered in a geometric fashion, generally a square. The output space is generally initialized with random values, with a relatively high learning rate, and also a relatively high number of epochs. The algorithm is made this way in order to be able to identify any type of data that can be represented as vectors.

First we will try to change the output space to better resemblance the social network. In order to do this, the squared grid that defines the output space was changed by the social network connections, and the neurons, are represented by a social network user. This changes are applied in the following way:
\begin{figure}[htpb]
  \centering
  \subfigure[The neighbourhood is defined by the relations of followers/followees between the winning neuron and the other neurons]{\includegraphics[scale=0.3]{./images/homophilic_outputspace.pdf}\label{chp3:homout}}
  \hspace*{0.5cm}
  \subfigure[Homophilic input space works in the same way as a normal input space]{\includegraphics[scale=0.3]{./images/homophilic_input_space.pdf}\label{chp3:homin}}
  \label{fig:homo_in_out}
  \caption{ Homophilic SOM output and input space during the learning phase. }
\end{figure}
\begin{itemize}
  \item Each neuron is comprised of the text from all the tweets that he authored.
  \item Each neuron has a unique id, and stores the ids of his followers and followees that are present in the output space.
  \item During the learning phase, the radius will be defined as the maximum number of hops separating the winning neuron and followers/followees of followers/followees. 
  %\item Each neuron will cache followers/followees of a follower/followee to a specified depth level, for performance purposes. 
\end{itemize}

{\color{red} insert image of the output space with social features vrs tipical output space}

\subsection{Learning Phase}
\label{sub:learning_phase}
Like in the default \ac{SOM} the learning phase is where the output space is trained in order to organize the input data into clusters. Since this algorithm is specific to categorize tweets using social network features, the learning rate, radius and number of epochs used can be greatly reduced in order for the algorithm to converge. The learning phase operates in the following way:

\begin{itemize}
  \item The distance between the input pattern and all the neurons is calculated. The neuron closest to the input pattern is considered the winning neuron.
  \item When the winning neuron is selected, he and his social neighbors within k hops, update their representations in the input space, and move closer to the input patter. The Gaussian function (Func.~\ref{eq:gaussian}) is also used in here in order for the neighbors that are closer to the input pattern be significantly more influenced by the input pattern, while the neurons further away are less influenced. 
  \item This process is repeated for a predefined number of epochs. While the number of epochs increases, the learning rate, and number of hops that defines the neighborhood decreases in order for the algorithm to converge.
\end{itemize}

Just like the default \ac{SOM} algorithm, after the map is trained, input patterns can be fast assign to the nearest neuron since the neuron positions in the output space are no longer updated.

{\color{red} Link to the learning phase in the algorithm on the main chapter, add images of the training model }
\subsection{Visualizing Neuron Representation Quality}
\label{sub:visualizing_neuron_representation_quality}
\input{./algorithms/qmatrix.tex}

\section{Social Clusters}
\label{sec:hmophilic_som_clusters}
{\color{red} resume what is written in this chapter }

\subsection{Training}
\label{sub:dataset}
In order to train the Homophilic SOM, we used the crawler defined in Section~\ref{sec:data_mining_in_twitter_}. The dataset had the following characteristics:
{\color{red} add table with number of users, tweets, tags, on the dataset}
{\color{red} show amount of time it took to train the SOM}
{\color{red} show umatrixes of the trainne }
{\color{red} compare clusters/time and umatrixes of the default SOM and the Homophilic SOM}
{\color{red} show the tweets in some clusters }


