\fancychapter{Evaluation Metrics}

\section{Clustering Tweets with Self-Organizing Maps}
\label{ch:clustering_tweets}

\subsection{SOM training}
\label{sub:clustering_tweets_with_soms}
Our first approach to cluster tweets with \ac{SOM} started by dynamically creating \ac{VSM} for each new word that was encountered while scanning each tweet. Due to simplicity of the approach, an overwhelming amount of different words, some, even without any clear meaning. 
Due to the huge amount \ac{VSM} size, the trainings at hand took an eternity to process, in order to prevent this from happening we took a sample of 50MB of tweets, all in English, from the dataset and started to train the \ac{SOM} with it. String manipulation for \ac{VSM} reduction described on Subsection~\ref{sec:reducing_som_vector_size} where used.
The \ac{SOM} training was performed using the R kohonen package~\cite{rsom}. We have added some information about this train on Appendix~\ref{ch:rsom_training}, three kinds of clusters where found. Clusters where no topic could be made sense of, clusters with a ton of tweets which had the same text and clusters that had more than one topic/no topics at all. An example of tweets presents in these clusters can be seen in Figures \ref{fig:cluster1}, \ref{fig:cluster2} and \ref{fig:cluster3}.   
 \begin{figure}[htpb]
  \centering
    \begin{boxedverbatim}
    iKitCloudKicker,guess its a twitter day at work
    mgdotorg,rick hunolt back in action for more than a song or two just saw this posted on exodus san fran
    ChoclateIceCian,such a rough and shit sleep lastnight has totally ruined my twitter buzz today l and tea isnt cheering me up o
    oooocherryoooo,received a postcrossing postcard from finland
    Carnage4Life,wil google is making a huge and annoying mistake
    earcario,this is a great invention simply via joleneo
    __missvalentine,guess my hair is going in a pony stupid rain
    iMacNoBook,it s crazy how twitter can make or break a relationship
    MattMullenUK,missed this yesterday google bigquery now out of trial mode brings bigdata for all at a price
AtlantaStreet,career work people are freaking out over a conservative whose twitter account keeps getting shut please retweet
    \end{boxedverbatim}
    \caption{Cluster without topics}
    \label{fig:cluster1}
\end{figure} 
\begin{figure}[htpb]
  \centering
    \begin{boxedverbatim}
    amanzana_com,la aplicaci n en la ipad o el iphone que un m dico debe chest journal diario m dico de la univerdad de stanford
    Avi_Fogel,nyangels ipad based look of day outfit sales for women bar rafaeli investor and model designers multiple stylist
    GaryPHayes,multi platform fantasy experience graphic novel ipad app augmented reality app film anomaly this oct pet
    iOSsociety,via instagram neon museum las vegas ios iphone ipad
    iznix,what s better than a protective case for iphone or ipad a case that can charge your device too solar cases are
    itine_utine,edwin andriann anak mu udiiin gak maen puzzle ntr tapi zaman ipad d pake maen tembing
    Visit_Newquay,we re selling our ipad new so if your in town its free delivery
    DigitalLyncher,if only spotify premium wasn t so pricy lol rt spotify finally arrives on ipad thanks to new universal update
    crunchy_f,i would love to get an ipad that s why i entered to win one
    landmarkpx,younger abstention doctrine kindle ipad landmarkpublications
    canadanielle,country music hey who s meowing who has an ipad
    dawsotron,anyone want to buy an ipad gb g locked to network my new ipad rd generation is being delivered tomorrow inbox me if interested
    CelinaAtFCM,linkedin launches a killer new ipad app business insider
    miramahira,arinasolehah hahaha pakcik die pun dah takleh wat pape kalu die dah conquer ipad tu
    GuitarChordz,spotify launches new ipad app with beautiful graphics gapless playback and more please retweet
    bahrainme,blackberry porsche design p apple iphone s ipad brand new original unlocked blackberry porsche desi
    ErikBernskiold,linkedin goes ipad native ios
    54111145,ipad z
    \end{boxedverbatim}
    \caption{Cluster filled with iPad tweets}
    \label{fig:cluster1}
\end{figure}
\begin{figure}[htpb]
  \centering
    \begin{boxedverbatim}                   
    studio44salon,i posted a new photo to facebook
    mynor1,i posted a new photo to facebook
    Sheikh_Basit,i posted a new photo to facebook
    Antalyarentacar,i posted a new photo to facebook
    sagitario1509_,i posted a new photo to facebook
    nishantgourav,i posted a new photo to facebook
    DISTINCT89,i posted a new photo to facebook
    markleeks,i posted a new photo to facebook
    bjbaloncio,i posted a new photo to facebook
    pastorbbg,i posted a new photo to facebook
    Christochelsea9,i posted a new photo to facebook
    ShaniChaudhary,i posted a new photo to facebook
    MaderaBrookside,i posted a new photo to facebook
    eveproducer,i posted a new photo to facebook
    Imamean1,i posted a new photo to facebook
    Peetje1960,i posted a new photo to facebook
    BmacMotorsport,i posted a new photo to facebook
    emilybr00ke,i posted a new photo to facebook
    StratfordSchool,i posted a new photo to facebook
    walkers4u,i posted a new photo to facebook
    sairussairus,i posted a new photo to facebook
    delifreshfood27,i posted a new photo to facebook
    igaurav430,i posted a new photo to facebook
    MyHolylandOrg,i posted a new photo to facebook
    saurav_saus,i posted a new photo to facebook
   
    \end{boxedverbatim}
    \caption{Cluster of \ac{CSV} tweets with very similar text}
    \label{fig:cluster3}
\end{figure}

\subsection{Reducing SOM vector size}
\label{sub:reducing_som_vector_size}
In Subsection~\ref{sub:clustering_tweets} we introduced string reducer methods which enabled great \ac{VSM} reduction. On Figure~\ref{fig:plot_word_red} we can see the amount of words removed by each method alone, and by all methods combined --- column "All Methods"---. In order to build this graph, we applied each method independently to a sample of 902802 tweets.

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.8\linewidth]{./plots/svm/plot_wordcount.pdf}
  \caption{Amount of unique words present on dataset sample with 902802 tweets, based on the string word reduction technique applied.}
  \label{fig:plot_word_red}
\end{figure}

It is interesting to see that each method by itself doesn't remove a great amount of words. The method that removed more words by itself was "remove non letters" --- which removes every character that is not a letter ---, at an order of 33\%. On the other hand, the method "remove stop words" by itself removed only 400 of words. This was expected due to the fact that the full list of MySQL stop words used by this method only has 543 words. All methods combined where able to reduce the \ac{VSM} size in about 75\%.

String reduction techniques work directly with text and has no notion whatsoever of linguistic semantics. I Subsection~\ref{sub:tweeter_natural_language_processing} we presented a twitter \ac{NLP} libray which can be used effectively to understand the linguistic semantics used on twitter. 

By feeding the same dataset as used above to the library we where able to identify multiple types of words that can afterwards be considered relevant for~\ac{TDT}. On Figure \ref{fig:plot_word_red}, the red bars show the amount of words unique words found under a specific semantic tag, whilst in red we can see words tagged under the same category after applying string reduction techniques.

Due to the fact that we are trying to identify topics, most of the tagged words are of no use. We chose to use only common nouns, proper nouns and hashtags during the clustering process. By applying all these filters to the dataset sample, we have a \ac{VSM} reduction of about 90\%, from 1 204 743 different words to 132 861.

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.8\linewidth]{./plots/svm/plot_wordcount_nlp.pdf}
  \caption{Number of words tagged with Ark Tweet NLP. In red we can see the number of words unique words tagged in each category, while in blue we can see the amount of unique words, after applying string reduction techniques.}
  \label{fig:wordcount_nlp}
\end{figure}

\subsubsection{Identify Tweets language}
\label{ssub:identify_tweets_lang}
Twitter is a social network with users from every corner of the world, and due to this fact, tweets tend to be in a lot of different languages which complicates the process of clustering due to increasing the number of sining ms, idiomatic expression and results interpretation due to lack of knowledge of a specific language.  
In order to find a tweets language, it is possible to see the language that a user has on his twitter profile through the twitter API, but these are not always the same thing. Some times, users use their profiles on different languages that the language in which they issue their tweets.
Identifying tweets that where not in the English language was done through the usage of Ruby library called whatlanguage~\footnote{https://github.com/peterc/whatlanguage}, which tries to identify one language through Bloom Filters. Inside the tweet there is a field which identifies the user language, we found that x is not acurate. Removing tweets that weren't in the english language reduced the amount of different words in x and therefor will reduce the dimensional size of the \ac{SOM}.
In order to prove that the using the whatlanguage library in conduction with the language which users have on twitter will improve the overall percentage of English written tweets, we took a random sample of 100 tweets from our own dataset, and categorized it by hand. Afterwards we compared the results of using only the language on the tweet, using the language detection library and both. The results are summarized on Table~\ref{tab:mac_test}.  
\input{tables/tweets_lang.tex}

Even though the sample is quite small, it was possible to understand that if both techniques where combined in order to detect the language of the tweet, the biggest problem which we could come across, would be to discard tweets that where not tagged as being in English, but in fact where. This is preferable, given that it easier to get more tweets in order to have English tweets, than constructing an \ac{VSM} with foreign terms.

\subsection{Conclusions}
\label{sub:conclusions}

\section{Twitter Crawler}
\label{sec:twitter_crawler}
  This problem could be solved in any of the following ways:
\begin{itemize}
  \item \textbf{First aproach: } For each tweet, fetch the user information including the users he is connected to.
  \item \textbf{Second aproach: } Create our own crawler, where the social connections, tweets and users are saved.
\end{itemize}                                                                                             

When designing the twitter crawler, we took into consideration that it had to be extremely resilient in order to be able to be left alone, crawling the twitter, until told to stop. Also if anything happened to the machine where the crawler was running it would be necessary to return to some previous crawling state, with minimum data loss. Algorithm {\color{red} I NEED TO WRITE THIS ALGORITHM } shows the crawler algorithm, which works in the following way:

\begin{itemize}
  \item \textbf{Step 1:} Choose some seed users to start crawling or deserialize a serialized version of the crawler if available.
  \item \textbf{Step 2:} For each seed user get all of his followers, and add them to an array if they haven't yet been crawled.
  \item \textbf{Step 3:} Repeat step one with random users taken from the array on step 2, until API limit is reached.
  \item \textbf{Step 4:} When API limit is reached, print the state of the crawled network, serialize the current state, and wait 15 minutes until it is possible to resume crawling. 
\end{itemize}

\subsection{Crawler Performance}
\label{sub:crawler_performance}
{\color{red} show number of tweets per second, number user per second, and users reached per second. }
{\color{red} compare the with the theoretical maximum }
{\color{red} compare size of the serialized social network with the amount of tweets and users it is getting  }
{\color{red} compare time taken to deserialize with amount of tweets crawled }
{\color{red} compare time to serialize with amount of tweets crawled }
 


\section{SOM Framework}
\label{sec:som_framework}
The \ac{SOM} framework was developed in the Ruby programing language \footnote{https://www.ruby-lang.org/en/} due to the desired characteristic of allowing great levels of introspection and being an almost pure object oriented programing language. Due to this characteristics making modifications to core parts of the algorithm is fairly easy.

The \ac{SOM} Framework was developed in a test driven fashion, having 100\% of its public methods tested and documented for expected behavior. These characteristics, associated with the fact that was published under an open source license, makes it available for other researchers to implement their own SOM variants.

By default, the base SOM algorithm is implemented as described by the Algorithm~\ref{alg:som} in Section \ref{sec:the_self_organizing_map}. 
\subsection{Clustering Color Vectors}
\label{sub:main_features}
Out of the box, the \ac{SOM} Framework implements a squared output space, where all residing neurons are manipulated as arrays. It is possible ate any given moment of the training to export the output space to \ac{JSON}, \ac{CSV} or to visualize its current \ac{U-Matrix}. Also during training a progress bar is displayed in order to know how much time will be needed for the training to end.

Due to the features described above, it is possible to train a \ac{SOM} to identify random colors --- RGB vectors --- while printing the results. In order to do this we will start by:
\begin{itemize}
  \item Initializing a SOM object with an output space size of 15 by 15 neurons, which will yield a total of 255 neurons --- and directly maps to the maximum number of clusters --- and 700 epochs.
  \item Create 1500 input patterns with size 3 and random values between 0 and 255. 
  \item Tell the SOM to print its state at the end of each epoch.
\end{itemize}
The machine used for training had the hardware specifications outlined in table \ref{tab:mac_test}

\input{./tables/mac.tex}

A summary of the training is specified in Table~\ref{tab:som_colours}
\input{./tables/som_colours.tex}

\begin{figure}[h]
  \includegraphics[scale=0.6]{./plots/som/topological_error.pdf}
  \label{fig:top_error}
  \caption{Changes in topological error throughout the SOM training, lrate stands for learning rate, and radius for radius applied to the winning neuron}
\end{figure}

\begin{figure}[h]
  \centerline{\includegraphics[scale=0.6]{./plots/som/average_distance.pdf}}
  \label{fig:avg_dist}
  \caption{Changes in the average distance between neurons, throughout the SOM training}
\end{figure}

\begin{figure}[htpb]
  \centering
  \subfigure[Output Space]{\includegraphics[scale=1]{./images/som_training/1_som.pdf}\label{chp3:onesom}}
  \hspace*{0.5cm}
  \subfigure[U-Matrix]{\includegraphics[scale=0.5]{./images/som_training/1_umatrix.pdf}\label{chp3:onematrix}}
  \hspace*{0.5cm}
  \subfigure[Q-Matrix]{\includegraphics[scale=1]{./images/som_training/1_quantmatrix.pdf}\label{chp3:onetopmat}}
  \hspace*{0.5cm}
  \caption{ SOM state after first epoch of training. Its learning rate is at 0.598, and radius at 8.  }
  \label{fig:}
\end{figure}

\begin{figure}[htpb]
  \centering
  \subfigure[Output Space]{\includegraphics[scale=1]{./images/som_training/2_som.pdf}\label{chp3:onesom}}
  \hspace*{0.5cm}
  \subfigure[U-Matrix]{\includegraphics[scale=0.5]{./images/som_training/2_umatrix.pdf}\label{chp3:onematrix}}
  \hspace*{0.5cm}
  \subfigure[Q-Matrix]{\includegraphics[scale=1]{./images/som_training/2_quantmatrix.pdf}\label{chp3:onetopmat}}
  \hspace*{0.5cm}
  \caption{ SOM state after second epoch of training. Its learning rate is at 0.22, and radius at 3.  }
  \label{fig:}
\end{figure}

\begin{figure}[htpb]
  \centering
  \subfigure[Output Space]{\includegraphics[scale=1]{./images/som_training/3_som.pdf}\label{chp3:onesom}}
  \hspace*{0.5cm}
  \subfigure[U-Matrix]{\includegraphics[scale=0.5]{./images/som_training/3_umatrix.pdf}\label{chp3:onematrix}}
  \hspace*{0.5cm}
  \subfigure[Q-Matrix]{\includegraphics[scale=1]{./images/som_training/3_quantmatrix.pdf}\label{chp3:onetopmat}}
  \hspace*{0.5cm}
  \caption{ SOM state after third epoch of training. Its learning rate is at 0.081, and radius at 1.  }
  \label{fig:}
\end{figure}

\begin{figure}[htpb]
  \centering
  \includegraphics[width=0.8\linewidth]{./images/som_trainned.pdf}
  \caption{Input patterns associated with the neuron with maximum topological error --31. Even though the neuron has the biggest topological error of all neurons, it still has a good representation of the input patterns. The colors in this image are not figurative, and represent the entities at the end of trainning  }
  \label{fig:./images/som_trainned}
\end{figure}

\subsection{Benchmarking}
\label{sub:benchmarking}



\section{Homophilic SOM}
\label{sec:homophilic_som}



\section{Conclusions}
{\color{red} Only these methods are not enough to cluster tweets }

% Ensure that the next chapter starts in a odd page
\cleardoublepage
 %\subsection{Twitter Dataset}
%\label{subsec:twitter_dataset}

%As can be seen in Figure~\ref{fig:json_tweet_user} no information about the social relations of the user which emitted the tweet are present. Therefor in order to retrieve the social network in which a user is contained, it will be necessary to connect to the Twitter API. Crawling twitter is discussed in further depth in Chapter~\ref{chap:crawling_twitter}.  


%In order to better understand the dataset at hand, all the \ac{JSON} files where converted into \ac{CSV}in a way to reduce the size of the dataset. While tweets where being converted, \ac{URL} where removed --- since most of them where minified in order to fit in less that 140 characters, without translating the minified URL, not a lot of information can be gathered. Also, all tweets that where not identified as being in English where also removed. The tweet shown in \ac{JSON} format in Figure~\ref{fig:json_tweet} is converted to \ac{CSV} in Figure~\ref{fig:csv_tweet}.
 
%%%%%% REMOVED STUFF
%\subsection{Topology Preservation} 
%\label{sub:topology_preservation}
%The Self-Organizing Map performs a mapping from the n-dimensional input space into the two dimensional output space and where resides one the most fascinating characteristics, which is that the output map tries to preserve the topology from the input space. This grants the SOM algorithm a way to visualize high-dimensional data that other neural networks or clustering algorithms don't have. Even though this is true, sometimes during training it is not possible to preserve the topology of the network.
%Thus topology preservation can be measured through the Topographic error~\citet{Kiviluoto1996} which is the proportion of all data vectors for which first and second BMUs \footnote{unit that is closest to the winning neuron. BMU Best fitting unit } are not adjacent units.
%In this project the Topographic Error will be calculated for all SOM implementations and VSM usages in order to understand if the representation of the SOM output space is well defined.

 %\begin{itemize}
  %\item show UMatrixes and multiple steps map trainning of the SOM library trainnig
  %\item show metrics for the crawller, tweets per second, users persecond, size of the dump a long the time.
  %\item compare my som library with other som libraries: training velocity with diferent parameters, map after trainned.
  %\item Compare Homophilic-SOM results with non homophilic: UMatrixes, cluster results, Quantization error, jacknife. 
%\end{itemize}
%%\section{Evaluation Metrics} 
%%\label{sec:evaluation_metrics}
%Evaluation of the topic detection on Tweets will be made in two distinct ways. The first way will focus on  binary classification using the precision and recall metrics, and will be described in Subsection~\ref{sub:testing_for_precision_and_recall}. The second way will focus on statistically testing the SOM learning process and the computed trained network. This testing process will be described in Subsection~\ref{sub:cluster_quality_testing}. 

%\section{Testing for Precision and Recall} 
%\label{sec:testing_for_precision_and_recall}
%Precision and Recall are both ways to measure the rate of right guesses made by the trained SOM network, and are defined in the following way:
%\begin{itemize}
  %\item \textbf{Precision:} Fraction of retrieved instances that where relevant 
    %\input{./equations/precision.tex}
  %\item \textbf{Recall:} Fraction of relevant instances that where retrieved
    %\input{./equations/recall.tex}
%\end{itemize}
 
%In order to calculate Precision and Recall we need to have the \emph{relevant documents} and the \emph{retrieved documents}. The \emph{relevant documents} are rather hard to determine because they need to be categorized by humans, which is an expensive task.
